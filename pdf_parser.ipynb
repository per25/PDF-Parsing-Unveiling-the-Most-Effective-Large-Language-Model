{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_decorator(func):\n",
    "    import time\n",
    "    import psutil\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        file_path = 'output_data/performance_metrics.xlsx'\n",
    "        file_name = args[0].split('/')[1].split('\\\\')[1].split('.')[0]\n",
    "        tool_name = func.__name__\n",
    "        start_time = time.time()\n",
    "        start_cpu = psutil.cpu_percent(interval=None)\n",
    "        start_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2\n",
    "\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "        end_time = time.time()\n",
    "        end_cpu = psutil.cpu_percent(interval=None)\n",
    "        end_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2\n",
    "\n",
    "        llm_tokens = np.nan\n",
    "        embedding_tokens = np.nan\n",
    "        pages_calls = np.nan\n",
    "\n",
    "        if isinstance(result, dict):  # is from llama index\n",
    "            llm_tokens = result.get('llm_tokens', 'N/A')\n",
    "            embedding_tokens = result.get('embedding_tokens', 'N/A')\n",
    "            pages_calls = result.get('pages_calls', 'N/A')\n",
    "\n",
    "        metrics = {\n",
    "            'Tool': [tool_name],\n",
    "            'file_name': [file_name],\n",
    "            'Execution Time (seconds)': [end_time - start_time],\n",
    "            'CPU Usage (percent)': [end_cpu - start_cpu],\n",
    "            'Memory Usage (MB)': [end_memory - start_memory],\n",
    "            'llm_tokens': [llm_tokens],\n",
    "            'embedding_tokens': [embedding_tokens],\n",
    "            'pages_calls': [pages_calls]\n",
    "        }\n",
    "        df = pd.DataFrame(metrics)\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if os.path.exists(file_path):\n",
    "            with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "                df.to_excel(writer, index=False, header=False, startrow=writer.sheets['Sheet1'].max_row)\n",
    "        else:\n",
    "            df.to_excel(file_path, index=False)\n",
    "\n",
    "        if isinstance(result, list):  # is from llama tesseract\n",
    "            for item in result:\n",
    "                df = pd.DataFrame(item, index=[0])  # Add index=[0] if item is a dictionary with scalar values\n",
    "                df.insert(0, 'file_name', file_name)\n",
    "                df.insert(0, 'Tool', tool_name)\n",
    "\n",
    "                # Check if the file already exists\n",
    "                if os.path.exists(file_path):\n",
    "                    with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "                        df.to_excel(writer, index=False, header=False, startrow=writer.sheets['Sheet1'].max_row)\n",
    "                else:\n",
    "                    df.to_excel(file_path, index=False)\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def get_number_of_pages(file_path):\n",
    "    # Open the PDF file\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        # Create PDF reader object\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        # Get the number of pages\n",
    "        number_of_pages = len(pdf_reader.pages)\n",
    "        return number_of_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_PyPDF(file_path, output_file):\n",
    "    from langchain_community.document_loaders import PyPDFLoader\n",
    "    \n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load_and_split()\n",
    "    \n",
    "    # Save the data to a text file for inspection\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        for i, page in enumerate(pages):\n",
    "            f.write(page.page_content)\n",
    "            f.write(\"\\n\")  # Optional: add a newline between pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_UnstructuredPDF_default_strategy(file_path, output_file):\n",
    "    from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "    loader = UnstructuredPDFLoader(file_path, mode=\"elements\")\n",
    "    pages = loader.load_and_split()\n",
    "    \n",
    "    # Save the data to a text file for inspection\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        for i, page in enumerate(pages):\n",
    "            f.write(page.page_content)\n",
    "            f.write(\"\\n\")  # Optional: add a newline between pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@performance_decorator\n",
    "def process_pdf_file_UnstructuredPDF_OCR_only_strategy(file_path, output_file):\n",
    "    from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "    loader = UnstructuredPDFLoader(file_path, mode=\"elements\", strategy='ocr_only')\n",
    "    pages = loader.load_and_split()\n",
    "    \n",
    "    # Save the data to a text file for inspection\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        for i, page in enumerate(pages):\n",
    "            f.write(page.page_content)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_UnstructuredPDF_hig_res_strategy(file_path, output_file):\n",
    "    from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "    loader = UnstructuredPDFLoader(file_path, mode=\"elements\", strategy='hi_res')\n",
    "    pages = loader.load_and_split()\n",
    "    \n",
    "    # Save the data to a text file for inspection\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        for i, page in enumerate(pages):\n",
    "            f.write(page.page_content)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_PDFMiner(file_path, output_file):\n",
    "    from langchain_community.document_loaders import PDFMinerLoader\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    data = loader.load()\n",
    "\n",
    "    # Save the data to a text file for inspection\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        for i, page in enumerate(data):\n",
    "            f.write(page.page_content)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_PDFMiner_as_HTML(file_path, output_file):\n",
    "    from langchain_community.document_loaders import PDFMinerPDFasHTMLLoader\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \n",
    "    loader = PDFMinerPDFasHTMLLoader(file_path)\n",
    "    data = loader.load()[0]   # entire PDF is loaded as a single Document\n",
    "    # print(data)\n",
    "    soup = BeautifulSoup(data.page_content,'html.parser')\n",
    "    content = soup.find_all('div')\n",
    "\n",
    "    # save the content to a html file \n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        f.write(str(content))\n",
    "\n",
    "    cur_fs = None\n",
    "    cur_text = ''\n",
    "    snippets = []   # first collect all snippets that have the same font size\n",
    "    for c in content:\n",
    "        sp = c.find('span')\n",
    "        if not sp:\n",
    "            continue\n",
    "        st = sp.get('style')\n",
    "        if not st:\n",
    "            continue\n",
    "        fs = re.findall('font-size:(\\d+)px',st)\n",
    "        if not fs:\n",
    "            continue\n",
    "        fs = int(fs[0])\n",
    "        if not cur_fs:\n",
    "            cur_fs = fs\n",
    "        if fs == cur_fs:\n",
    "            cur_text += c.text\n",
    "        else:\n",
    "            snippets.append((cur_text,cur_fs))\n",
    "            cur_fs = fs\n",
    "            cur_text = c.text\n",
    "    snippets.append((cur_text,cur_fs))\n",
    "\n",
    "    # print the snippets\n",
    "    for s in snippets:\n",
    "        # print(s)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_PyMuPDF(file_path, output_file):\n",
    "    import fitz as PyMuPDF\n",
    "    # TODO Add the ocr \n",
    "\n",
    "    doc = PyMuPDF.open(file_path) # open a document\n",
    "    \n",
    "    out = open(output_file, \"wb\") # create a text output\n",
    "    \n",
    "    for page in doc: # iterate the document pages\n",
    "        text = page.get_text().encode(\"utf8\") # get plain text (is in UTF-8)\n",
    "        out.write(text) # write text of page\n",
    "        out.write(bytes((12,))) # write page delimiter (form feed 0x0C)\n",
    "    \n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_pdfminerSix(file_path, output_file):\n",
    "    from io import StringIO\n",
    "    from pdfminer.high_level import extract_text_to_fp\n",
    "    from pdfminer.layout import LAParams\n",
    "\n",
    "    output_string = StringIO()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        extract_text_to_fp(f, output_string, laparams=LAParams())\n",
    "\n",
    "    # Save the data to a text file for inspection\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        f.write(output_string.getvalue().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_textract_with_correction(file_path, output_file):\n",
    "    import tesseract_with_llama2_corrections as tesseract_with_llama2\n",
    "    raw_ocr, corrected_text, filter_text, performance_metrics  = tesseract_with_llama2.tesseract_with_llm_correction(file_path)\n",
    "    with open(output_file + \"_raw_ocr.md\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(raw_ocr)\n",
    "    with open(output_file + \"_corrected\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(corrected_text)\n",
    "    with open(output_file + \"_fileted.md\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(filter_text)\n",
    "    \n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_llama_index_md(file_path, output_file):\n",
    "    import nest_asyncio \n",
    "    from llama_parse import LlamaParse\n",
    "    from os import getenv\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    key = getenv(\"LlamaIndex\")\n",
    "\n",
    "    parser = LlamaParse(\n",
    "        api_key=key,\n",
    "        result_type=\"markdown\",\n",
    "        num_workers=4,\n",
    "        verbose=True,\n",
    "        language=\"en\"\n",
    "    )\n",
    "\n",
    "    # sync \n",
    "    document = parser.load_data(file_path)\n",
    "\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        f.write(document[0].text)\n",
    "    \n",
    "    return {'pages_calls': get_number_of_pages(file_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_llama_index_txt(file_path, output_file):\n",
    "    import nest_asyncio \n",
    "    from llama_parse import LlamaParse\n",
    "    from os import getenv\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    key = getenv(\"LlamaIndex\")\n",
    "\n",
    "    parser = LlamaParse(\n",
    "        api_key=key,\n",
    "        result_type=\"text\",\n",
    "        num_workers=4,\n",
    "        verbose=True,\n",
    "        language=\"en\"\n",
    "    )\n",
    "\n",
    "    # sync \n",
    "    document = parser.load_data(file_path)\n",
    "    \n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        f.write(document[0].text)\n",
    "\n",
    "    return {'pages_calls': get_number_of_pages(file_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_performance_metrics_file():\n",
    "    \"\"\"\n",
    "    Resets the performance metrics file by clearing its contents and adding a header.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    with open('output_data/performance_metrics.txt', 'w') as f:\n",
    "        f.write(\"Performance metrics for each function:\\n\\n\")\n",
    "\n",
    "    # clear the excel file\n",
    "    if os.path.exists('output_data/performance_metrics.xlsx'):\n",
    "        os.remove('output_data/performance_metrics.xlsx')\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.DataFrame(columns=['Tool', 'File', 'Execution Time (seconds)', 'CPU Usage (percent)', 'Memory Usage (MB)', 'llm_tokens', 'embedding_tokens', 'pages_calls'])\n",
    "    df.to_excel('output_data/performance_metrics.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_output_files(path):\n",
    "    \"\"\"\n",
    "    Clears all the files and folders in path directory with extensions '.txt', '.html', and '.md'.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".txt\") or file.endswith(\".html\") or file.endswith(\".md\"):\n",
    "            os.remove(os.path.join(path, file))\n",
    "        elif os.path.isdir(os.path.join(path, file)):\n",
    "            shutil.rmtree(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  mixed1.pdf\n",
      "output_path:  output_data\\mixed1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\mixed1\\Unstructured_hi_res.txt:   9%|▉         | 1/11 [00:02<00:23,  2.33s/it]This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "Processing output_data\\mixed1\\textract:  73%|███████▎  | 8/11 [00:46<00:07,  2.41s/it]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now converting all pages of PDF file input_data/pdf\\mixed1.pdf to images...\n",
      "Done converting pages from PDF file input_data/pdf\\mixed1.pdf to images.\n",
      "Tesseract version: 5.3.3.20231005\n",
      "Extracting text from converted pages...\n",
      "Processing page 1 with LLM...\n",
      "Processing page 2 with LLM...\n",
      "Now filtering out hallucinations from corrected text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\mixed1\\llama_index_md.md:  82%|████████▏ | 9/11 [01:40<00:36, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done filtering out hallucinations.\n",
      "Started parsing the file under job_id cca4bfa5-64bd-440e-a09e-c07b36143c53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\mixed1\\llama_index_txt.txt:  91%|█████████ | 10/11 [01:46<00:14, 14.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 17213e77-2331-4a8c-9133-793ecc7eb4b9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\mixed1\\llama_index_txt.txt: 100%|██████████| 11/11 [01:50<00:00, 10.01s/it]\n"
     ]
    }
   ],
   "source": [
    "def run_all(input_folder_path, output_folder_path) -> None:\n",
    "    \"\"\"\n",
    "    Runs all the PDF processing functions and saves the output to respective files.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # get all the files in the folder\n",
    "    files = os.listdir(input_folder_path)\n",
    "    \n",
    "    clear_output_files(output_folder_path)\n",
    "    reset_performance_metrics_file()\n",
    "\n",
    "    for file in files:\n",
    "        print(\"file: \", file)\n",
    "        output_path = os.path.join(output_folder_path, file.split(\".\")[0])\n",
    "        input_path = os.path.join(input_folder_path, file)\n",
    "        print(\"output_path: \", output_path)\n",
    "        # create a folder for the output data in the output folder\n",
    "        if not os.path.exists(file.split(\".\")[0]):\n",
    "            path = os.path.join(output_folder_path, file.split(\".\")[0])\n",
    "            os.makedirs(path)\n",
    "\n",
    "        tasks = [\n",
    "            (process_pdf_file_PyPDF, (input_path, os.path.join(output_path, \"PyPDF.txt\"))),\n",
    "            (process_pdf_file_UnstructuredPDF_hig_res_strategy, (input_path, os.path.join(output_path, \"Unstructured_hi_res.txt\"))),\n",
    "            (process_pdf_file_UnstructuredPDF_default_strategy, (input_path, os.path.join(output_path, \"Unstructured.txt\"))),\n",
    "            (process_pdf_file_UnstructuredPDF_OCR_only_strategy, (input_path, os.path.join(output_path, \"Unstructured_OCR.txt\"))),\n",
    "            (process_pdf_file_PDFMiner, (input_path, os.path.join(output_path, \"PDFMiner.txt\"))),\n",
    "            (process_pdf_file_PDFMiner_as_HTML, (input_path, os.path.join(output_path, \"PDFMiner_HTML.html\"))),\n",
    "            (process_pdf_file_PyMuPDF, (input_path, os.path.join(output_path, \"PyMuPDF.txt\"))),\n",
    "            (process_pdf_file_pdfminerSix, (input_path, os.path.join(output_path, \"pdfminerSix.txt\"))),\n",
    "            (process_pdf_file_textract_with_correction, (input_path, os.path.join(output_path, \"textract\"))),       \n",
    "            (process_pdf_file_llama_index_md, (input_path, os.path.join(output_path, \"llama_index_md.md\"))),\n",
    "            (process_pdf_file_llama_index_txt, (input_path, os.path.join(output_path, \"llama_index_txt.txt\")))\n",
    "        ]\n",
    "\n",
    "        with tqdm(total=len(tasks)) as pbar:\n",
    "            for task in tasks:\n",
    "                func, args = task\n",
    "                pbar.set_description(f\"Processing {args[1]}\")\n",
    "                try:\n",
    "                    func(*args)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nAn error occurred while processing {args[1]}: {str(e)}\")\n",
    "                    print(\"Press any key to continue...\")\n",
    "                    input()\n",
    "                pbar.update()\n",
    "\n",
    "\n",
    "run_all(\"input_data/pdf\", \"output_data\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
