{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_decorator(func):\n",
    "    \"\"\"\n",
    "    A decorator that measures the performance metrics of a function and saves them to an Excel file.\n",
    "\n",
    "    Args:\n",
    "        func (function): The function to be decorated.\n",
    "\n",
    "    Returns:\n",
    "        function: The decorated function.\n",
    "\n",
    "    \"\"\"\n",
    "    import time\n",
    "    import psutil\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        file_path = 'output_data/performance_metrics.xlsx'\n",
    "        print(args[0])\n",
    "        file_name = args[0].split('/')[1].split('\\\\')[1].split('.')[0]\n",
    "        tool_name = func.__name__\n",
    "        start_time = time.time()\n",
    "        start_cpu = psutil.cpu_percent(interval=None)\n",
    "        start_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2\n",
    "\n",
    "        result = func(*args, **kwargs)\n",
    "\n",
    "        end_time = time.time()\n",
    "        end_cpu = psutil.cpu_percent(interval=None)\n",
    "        end_memory = psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2\n",
    "\n",
    "        if result is None: # is from the rest of them \n",
    "            # the additional metrics is a dict that have that can be llm_tokens, embedding_tokens and pages calls or just one of them\n",
    "            llm_tokens = np.nan\n",
    "            embedding_tokens = np.nan\n",
    "            pages_calls = np.nan\n",
    "\n",
    "            metrics = {\n",
    "                'Tool': [tool_name],\n",
    "                'file_name': [file_name],\n",
    "                'Execution Time (seconds)': [end_time - start_time],\n",
    "                'CPU Usage (percent)': [end_cpu - start_cpu],\n",
    "                'Memory Usage (MB)': [end_memory - start_memory],\n",
    "                'llm_tokens': [llm_tokens],\n",
    "                'embedding_tokens': [embedding_tokens],\n",
    "                'pages_calls': [pages_calls]\n",
    "            }\n",
    "            df = pd.DataFrame(metrics)\n",
    "\n",
    "            # Check if the file already exists\n",
    "            if os.path.exists(file_path):\n",
    "                with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "                    df.to_excel(writer, index=False, header=False, startrow=writer.sheets['Sheet1'].max_row)\n",
    "            else:\n",
    "                df.to_excel(file_path, index=False)\n",
    "\n",
    "        elif isinstance(result, list): # is from llama tesseract\n",
    "            for item in result:\n",
    "                df = pd.DataFrame(item, index=[0])  # Add index=[0] if item is a dictionary with scalar values\n",
    "                df.insert(0, 'file_name', file_name)\n",
    "                df.insert(0, 'Tool', tool_name)\n",
    "\n",
    "                # Check if the file already exists\n",
    "                if os.path.exists(file_path):\n",
    "                    with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "                        df.to_excel(writer, index=False, header=False, startrow=writer.sheets['Sheet1'].max_row)\n",
    "                else:\n",
    "                    df.to_excel(file_path, index=False)\n",
    "    \n",
    "        else: # is from llama index \n",
    "                        # the additional metrics is a dict that have that can be llm_tokens, embedding_tokens and pages calls or just one of them\n",
    "            llm_tokens = result.get('llm_tokens', 'N/A')\n",
    "            embedding_tokens = result.get('embedding_tokens', 'N/A')\n",
    "            pages_calls = result.get('pages_calls', 'N/A')\n",
    "\n",
    "            metrics = {\n",
    "                'Tool': [tool_name],\n",
    "                'file_name': [file_name],\n",
    "                'Execution Time (seconds)': [end_time - start_time],\n",
    "                'CPU Usage (percent)': [end_cpu - start_cpu],\n",
    "                'Memory Usage (MB)': [end_memory - start_memory],\n",
    "                'llm_tokens': [llm_tokens],\n",
    "                'embedding_tokens': [embedding_tokens],\n",
    "                'pages_calls': [pages_calls]\n",
    "            }\n",
    "            df = pd.DataFrame(metrics)\n",
    "\n",
    "            # Check if the file already exists\n",
    "            if os.path.exists(file_path):\n",
    "                with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "                    df.to_excel(writer, index=False, header=False, startrow=writer.sheets['Sheet1'].max_row)\n",
    "            else:\n",
    "                df.to_excel(file_path, index=False)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def get_number_of_pages(file_path):\n",
    "    # Open the PDF file\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        # Create PDF reader object\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        # Get the number of pages\n",
    "        number_of_pages = len(pdf_reader.pages)\n",
    "        return number_of_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_PyPDF(file_path, output_file):\n",
    "    from langchain_community.document_loaders import PyPDFLoader\n",
    "    \n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load_and_split()\n",
    "    \n",
    "    # Save the data to a text file for inspection\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        for i, page in enumerate(pages):\n",
    "            f.write(page.page_content)\n",
    "            f.write(\"\\n\")  # Optional: add a newline between pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_UnstructuredPDF_default_strategy(file_path, output_file):\n",
    "    from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "    loader = UnstructuredPDFLoader(file_path, mode=\"elements\")\n",
    "    pages = loader.load_and_split()\n",
    "    \n",
    "    # Save the data to a text file for inspection\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        for i, page in enumerate(pages):\n",
    "            f.write(page.page_content)\n",
    "            f.write(\"\\n\")  # Optional: add a newline between pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@performance_decorator\n",
    "def process_pdf_file_UnstructuredPDF_OCR_only_strategy(file_path, output_file):\n",
    "    from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "    loader = UnstructuredPDFLoader(file_path, mode=\"elements\", strategy='ocr_only')\n",
    "    pages = loader.load_and_split()\n",
    "    \n",
    "    # Save the data to a text file for inspection\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        for i, page in enumerate(pages):\n",
    "            f.write(page.page_content)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_UnstructuredPDF_hig_res_strategy(file_path, output_file):\n",
    "    from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "    loader = UnstructuredPDFLoader(file_path, mode=\"elements\", strategy='hi_res')\n",
    "    pages = loader.load_and_split()\n",
    "    \n",
    "    # Save the data to a text file for inspection\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        for i, page in enumerate(pages):\n",
    "            f.write(page.page_content)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_PDFMiner(file_path, output_file):\n",
    "    from langchain_community.document_loaders import PDFMinerLoader\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    data = loader.load()\n",
    "\n",
    "    # Save the data to a text file for inspection\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        for i, page in enumerate(data):\n",
    "            f.write(page.page_content)\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_PDFMiner_as_HTML(file_path, output_file):\n",
    "    from langchain_community.document_loaders import PDFMinerPDFasHTMLLoader\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "    \n",
    "    loader = PDFMinerPDFasHTMLLoader(file_path)\n",
    "    data = loader.load()[0]   # entire PDF is loaded as a single Document\n",
    "    # print(data)\n",
    "    soup = BeautifulSoup(data.page_content,'html.parser')\n",
    "    content = soup.find_all('div')\n",
    "\n",
    "    # save the content to a html file \n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        f.write(str(content))\n",
    "\n",
    "    cur_fs = None\n",
    "    cur_text = ''\n",
    "    snippets = []   # first collect all snippets that have the same font size\n",
    "    for c in content:\n",
    "        sp = c.find('span')\n",
    "        if not sp:\n",
    "            continue\n",
    "        st = sp.get('style')\n",
    "        if not st:\n",
    "            continue\n",
    "        fs = re.findall('font-size:(\\d+)px',st)\n",
    "        if not fs:\n",
    "            continue\n",
    "        fs = int(fs[0])\n",
    "        if not cur_fs:\n",
    "            cur_fs = fs\n",
    "        if fs == cur_fs:\n",
    "            cur_text += c.text\n",
    "        else:\n",
    "            snippets.append((cur_text,cur_fs))\n",
    "            cur_fs = fs\n",
    "            cur_text = c.text\n",
    "    snippets.append((cur_text,cur_fs))\n",
    "\n",
    "    # print the snippets\n",
    "    for s in snippets:\n",
    "        # print(s)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_PyMuPDF(file_path, output_file):\n",
    "    import fitz as PyMuPDF\n",
    "    # TODO Add the ocr \n",
    "\n",
    "    doc = PyMuPDF.open(file_path) # open a document\n",
    "    \n",
    "    out = open(output_file, \"wb\") # create a text output\n",
    "    \n",
    "    for page in doc: # iterate the document pages\n",
    "        text = page.get_text().encode(\"utf8\") # get plain text (is in UTF-8)\n",
    "        out.write(text) # write text of page\n",
    "        out.write(bytes((12,))) # write page delimiter (form feed 0x0C)\n",
    "    \n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_pdfminerSix(file_path, output_file):\n",
    "    from io import StringIO\n",
    "    from pdfminer.high_level import extract_text_to_fp\n",
    "    from pdfminer.layout import LAParams\n",
    "\n",
    "    output_string = StringIO()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        extract_text_to_fp(f, output_string, laparams=LAParams())\n",
    "\n",
    "    # Save the data to a text file for inspection\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        f.write(output_string.getvalue().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_textract_with_correction(file_path, output_file):\n",
    "    import tesseract_with_llama2_corrections as tesseract_with_llama2\n",
    "    raw_ocr, corrected_text, filter_text, performance_metrics  = tesseract_with_llama2.tesseract_with_llm_correction(file_path)\n",
    "    with open(output_file + \"_raw_ocr.md\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(raw_ocr)\n",
    "    with open(output_file + \"_corrected\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(corrected_text)\n",
    "    with open(output_file + \"_fileted.md\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(filter_text)\n",
    "    \n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_llama_index_md(file_path, output_file):\n",
    "    import nest_asyncio \n",
    "    from llama_parse import LlamaParse\n",
    "    from os import getenv\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    key = getenv(\"LlamaIndex\")\n",
    "\n",
    "    parser = LlamaParse(\n",
    "        api_key=key,\n",
    "        result_type=\"markdown\",\n",
    "        num_workers=4,\n",
    "        verbose=True,\n",
    "        language=\"en\"\n",
    "    )\n",
    "\n",
    "    # sync \n",
    "    document = parser.load_data(file_path)\n",
    "\n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        f.write(document[0].text)\n",
    "    \n",
    "    return {'pages_calls': get_number_of_pages(file_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@performance_decorator\n",
    "def process_pdf_file_llama_index_txt(file_path, output_file):\n",
    "    import nest_asyncio \n",
    "    from llama_parse import LlamaParse\n",
    "    from os import getenv\n",
    "\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    key = getenv(\"LlamaIndex\")\n",
    "\n",
    "    parser = LlamaParse(\n",
    "        api_key=key,\n",
    "        result_type=\"text\",\n",
    "        num_workers=4,\n",
    "        verbose=True,\n",
    "        language=\"en\"\n",
    "    )\n",
    "\n",
    "    # sync \n",
    "    document = parser.load_data(file_path)\n",
    "    \n",
    "    with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "        f.write(document[0].text)\n",
    "\n",
    "    return {'pages_calls': get_number_of_pages(file_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_performance_metrics_file():\n",
    "    \"\"\"\n",
    "    Resets the performance metrics file by clearing its contents and adding a header.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    with open('output_data/performance_metrics.txt', 'w') as f:\n",
    "        f.write(\"Performance metrics for each function:\\n\\n\")\n",
    "\n",
    "    # clear the excel file\n",
    "    if os.path.exists('output_data/performance_metrics.xlsx'):\n",
    "        os.remove('output_data/performance_metrics.xlsx')\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    df = pd.DataFrame(columns=['Tool', 'File', 'Execution Time (seconds)', 'CPU Usage (percent)', 'Memory Usage (MB)', 'llm_tokens', 'embedding_tokens', 'pages_calls'])\n",
    "    df.to_excel('output_data/performance_metrics.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_output_files(path):\n",
    "    \"\"\"\n",
    "    Clears all the files and folders in path directory with extensions '.txt', '.html', and '.md'.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import shutil\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".txt\") or file.endswith(\".html\") or file.endswith(\".md\"):\n",
    "            os.remove(os.path.join(path, file))\n",
    "        elif os.path.isdir(os.path.join(path, file)):\n",
    "            shutil.rmtree(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  aidonHanInterface.pdf\n",
      "output_path:  output_data\\aidonHanInterface\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\aidonHanInterface\\PyPDF.txt:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\aidonHanInterface.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\aidonHanInterface\\Unstructured_hi_res.txt:   9%|▉         | 1/11 [00:01<00:11,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\aidonHanInterface.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "Processing output_data\\aidonHanInterface\\Unstructured.txt:  18%|█▊        | 2/11 [00:32<02:49, 18.88s/it]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\aidonHanInterface.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\aidonHanInterface\\Unstructured_OCR.txt:  27%|██▋       | 3/11 [00:34<01:27, 10.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\aidonHanInterface.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\aidonHanInterface\\PDFMiner.txt:  36%|███▋      | 4/11 [01:06<02:15, 19.40s/it]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\aidonHanInterface.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\aidonHanInterface\\PDFMiner_HTML.html:  45%|████▌     | 5/11 [01:07<01:17, 12.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\aidonHanInterface.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\aidonHanInterface\\pdfminerSix.txt:  64%|██████▎   | 7/11 [01:09<00:36,  9.02s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\aidonHanInterface.pdf\n",
      "input_data/pdf\\aidonHanInterface.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\aidonHanInterface\\textract:  73%|███████▎  | 8/11 [01:10<00:14,  4.90s/it]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\aidonHanInterface.pdf\n",
      "Now converting all pages of PDF file input_data/pdf\\aidonHanInterface.pdf to images...\n",
      "Done converting pages from PDF file input_data/pdf\\aidonHanInterface.pdf to images.\n",
      "Tesseract version: 5.3.3.20231005\n",
      "Extracting text from converted pages...\n",
      "Processing page 1 with LLM...\n",
      "Processing page 2 with LLM...\n",
      "Processing page 3 with LLM...\n",
      "Processing page 4 with LLM...\n",
      "Processing page 5 with LLM...\n",
      "Processing page 6 with LLM...\n",
      "Processing page 7 with LLM...\n",
      "Processing page 8 with LLM...\n",
      "Processing page 9 with LLM...\n",
      "Now filtering out hallucinations from corrected text...\n",
      "No existing database found at ./sentence_embeddings.sqlite. Creating a new one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\aidonHanInterface\\llama_index.md:  82%|████████▏ | 9/11 [02:53<01:00, 30.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done filtering out hallucinations.\n",
      "input_data/pdf\\aidonHanInterface.pdf\n",
      "Started parsing the file under job_id 461bb80d-52b3-4861-815b-2e977e95f3c2\n",
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\aidonHanInterface\\llama_index.txt:  91%|█████████ | 10/11 [03:28<00:31, 31.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\aidonHanInterface.pdf\n",
      "Started parsing the file under job_id 5c1fadc8-ad1c-40f8-a6aa-4b9edcbec19c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\aidonHanInterface\\llama_index.txt: 100%|██████████| 11/11 [03:33<00:00, 19.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  images.pdf\n",
      "output_path:  output_data\\images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\images\\Unstructured_hi_res.txt:   9%|▉         | 1/11 [00:00<00:01,  7.63it/s]This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\images.pdf\n",
      "input_data/pdf\\images.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\images\\Unstructured.txt:  18%|█▊        | 2/11 [00:13<01:12,  8.11s/it]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\images.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\images\\Unstructured_OCR.txt:  27%|██▋       | 3/11 [00:28<01:30, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\images.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\images\\textract:  73%|███████▎  | 8/11 [00:44<00:14,  4.76s/it]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\images.pdf\n",
      "input_data/pdf\\images.pdf\n",
      "input_data/pdf\\images.pdf\n",
      "input_data/pdf\\images.pdf\n",
      "input_data/pdf\\images.pdf\n",
      "Now converting all pages of PDF file input_data/pdf\\images.pdf to images...\n",
      "Done converting pages from PDF file input_data/pdf\\images.pdf to images.\n",
      "Tesseract version: 5.3.3.20231005\n",
      "Extracting text from converted pages...\n",
      "Processing page 1 with LLM...\n",
      "Processing page 2 with LLM...\n",
      "Processing page 3 with LLM...\n",
      "Processing page 4 with LLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\images\\textract:  73%|███████▎  | 8/11 [01:03<00:14,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now filtering out hallucinations from corrected text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\images\\llama_index.md:  82%|████████▏ | 9/11 [01:22<00:20, 10.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done filtering out hallucinations.\n",
      "input_data/pdf\\images.pdf\n",
      "Started parsing the file under job_id fcd31e9c-c61e-4259-8b69-987bfb652fa8\n",
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\images\\llama_index.txt:  91%|█████████ | 10/11 [02:14<00:18, 18.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\images.pdf\n",
      "Started parsing the file under job_id d45ed79d-5ba0-478c-b33c-ad2e0e9b0488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\images\\llama_index.txt: 100%|██████████| 11/11 [02:19<00:00, 12.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  mixed1.pdf\n",
      "output_path:  output_data\\mixed1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\mixed1\\PyPDF.txt:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\mixed1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\mixed1\\Unstructured_hi_res.txt:   9%|▉         | 1/11 [00:00<00:02,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\mixed1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "Processing output_data\\mixed1\\Unstructured.txt:  18%|█▊        | 2/11 [00:10<00:55,  6.13s/it]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\mixed1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\mixed1\\Unstructured_OCR.txt:  27%|██▋       | 3/11 [00:10<00:27,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\mixed1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\mixed1\\PDFMiner.txt:  36%|███▋      | 4/11 [00:23<00:48,  6.95s/it]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\mixed1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\mixed1\\PDFMiner_HTML.html:  45%|████▌     | 5/11 [00:23<00:27,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\mixed1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\mixed1\\pdfminerSix.txt:  64%|██████▎   | 7/11 [00:23<00:12,  3.14s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\mixed1.pdf\n",
      "input_data/pdf\\mixed1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\mixed1\\textract:  73%|███████▎  | 8/11 [00:24<00:05,  1.69s/it]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\mixed1.pdf\n",
      "Now converting all pages of PDF file input_data/pdf\\mixed1.pdf to images...\n",
      "Done converting pages from PDF file input_data/pdf\\mixed1.pdf to images.\n",
      "Tesseract version: 5.3.3.20231005\n",
      "Extracting text from converted pages...\n",
      "Processing page 1 with LLM...\n",
      "Processing page 2 with LLM...\n",
      "Now filtering out hallucinations from corrected text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\mixed1\\llama_index.md:  82%|████████▏ | 9/11 [01:47<00:45, 22.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done filtering out hallucinations.\n",
      "input_data/pdf\\mixed1.pdf\n",
      "Started parsing the file under job_id 50fa6c63-36ec-4b8b-92e9-d452abfddf7a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\mixed1\\llama_index.txt:  91%|█████████ | 10/11 [02:09<00:22, 22.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\mixed1.pdf\n",
      "Started parsing the file under job_id ecd41655-2314-4b2e-8f0c-243c66992d16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\mixed1\\llama_index.txt: 100%|██████████| 11/11 [02:14<00:00, 12.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  tables.pdf\n",
      "output_path:  output_data\\tables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\tables\\PyPDF.txt:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\tables.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\tables\\Unstructured_hi_res.txt:   9%|▉         | 1/11 [00:01<00:12,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\tables.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "Processing output_data\\tables\\Unstructured.txt:  18%|█▊        | 2/11 [00:15<01:20,  8.99s/it]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\tables.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\tables\\Unstructured_OCR.txt:  27%|██▋       | 3/11 [00:17<00:45,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\tables.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\tables\\PDFMiner.txt:  36%|███▋      | 4/11 [00:30<00:59,  8.57s/it]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\tables.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\tables\\PDFMiner_HTML.html:  45%|████▌     | 5/11 [00:31<00:36,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\tables.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\tables\\pdfminerSix.txt:  64%|██████▎   | 7/11 [00:33<00:17,  4.48s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\tables.pdf\n",
      "input_data/pdf\\tables.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\tables\\textract:  73%|███████▎  | 8/11 [00:34<00:07,  2.56s/it]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\tables.pdf\n",
      "Now converting all pages of PDF file input_data/pdf\\tables.pdf to images...\n",
      "Done converting pages from PDF file input_data/pdf\\tables.pdf to images.\n",
      "Tesseract version: 5.3.3.20231005\n",
      "Extracting text from converted pages...\n",
      "Processing page 1 with LLM...\n",
      "Processing page 2 with LLM...\n",
      "Processing page 3 with LLM...\n",
      "Processing page 4 with LLM...\n",
      "Now filtering out hallucinations from corrected text...\n",
      "Done filtering out hallucinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\tables\\llama_index.md:  82%|████████▏ | 9/11 [01:16<00:25, 12.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\tables.pdf\n",
      "Started parsing the file under job_id ae51b559-9fd4-4d18-bfdb-16e593a120f6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\tables\\llama_index.txt:  91%|█████████ | 10/11 [01:45<00:17, 17.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\tables.pdf\n",
      "Started parsing the file under job_id c5eb2a83-af41-473e-8c63-a0bb0acb6765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\tables\\llama_index.txt: 100%|██████████| 11/11 [01:51<00:00, 10.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  texts.pdf\n",
      "output_path:  output_data\\texts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\texts\\PyPDF.txt:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\texts.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\texts\\Unstructured_hi_res.txt:   9%|▉         | 1/11 [00:00<00:03,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\texts.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "Processing output_data\\texts\\Unstructured.txt:  18%|█▊        | 2/11 [00:16<01:25,  9.53s/it]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\texts.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\texts\\Unstructured_OCR.txt:  27%|██▋       | 3/11 [00:16<00:43,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\texts.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\texts\\PDFMiner.txt:  36%|███▋      | 4/11 [00:34<01:12, 10.39s/it]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\texts.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\texts\\PDFMiner_HTML.html:  45%|████▌     | 5/11 [00:35<00:40,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\texts.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\texts\\pdfminerSix.txt:  64%|██████▎   | 7/11 [00:35<00:18,  4.68s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\texts.pdf\n",
      "input_data/pdf\\texts.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\texts\\textract:  73%|███████▎  | 8/11 [00:36<00:07,  2.51s/it]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\texts.pdf\n",
      "Now converting all pages of PDF file input_data/pdf\\texts.pdf to images...\n",
      "Done converting pages from PDF file input_data/pdf\\texts.pdf to images.\n",
      "Tesseract version: 5.3.3.20231005\n",
      "Extracting text from converted pages...\n",
      "Processing page 1 with LLM...\n",
      "Processing page 2 with LLM...\n",
      "Processing page 3 with LLM...\n",
      "Processing page 4 with LLM...\n",
      "Now filtering out hallucinations from corrected text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\texts\\llama_index.md:  82%|████████▏ | 9/11 [01:57<00:46, 23.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done filtering out hallucinations.\n",
      "input_data/pdf\\texts.pdf\n",
      "Started parsing the file under job_id d5de99f3-dfaa-4d74-9ae6-d1e4649bc930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\texts\\llama_index.txt:  91%|█████████ | 10/11 [02:02<00:18, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_data/pdf\\texts.pdf\n",
      "Started parsing the file under job_id 224cf00a-6f85-4bc9-b29d-0a4f87d48301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing output_data\\texts\\llama_index.txt: 100%|██████████| 11/11 [02:07<00:00, 11.59s/it]\n"
     ]
    }
   ],
   "source": [
    "def run_all(input_folder_path, output_folder_path) -> None:\n",
    "    \"\"\"\n",
    "    Runs all the PDF processing functions and saves the output to respective files.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # get all the files in the folder\n",
    "    files = os.listdir(input_folder_path)\n",
    "    \n",
    "    clear_output_files(output_folder_path)\n",
    "    reset_performance_metrics_file()\n",
    "\n",
    "    for file in files:\n",
    "        print(\"file: \", file)\n",
    "        output_path = os.path.join(output_folder_path, file.split(\".\")[0])\n",
    "        input_path = os.path.join(input_folder_path, file)\n",
    "        print(\"output_path: \", output_path)\n",
    "        # create a folder for the output data in the output folder\n",
    "        if not os.path.exists(file.split(\".\")[0]):\n",
    "            path = os.path.join(output_folder_path, file.split(\".\")[0])\n",
    "            os.makedirs(path)\n",
    "\n",
    "        tasks = [\n",
    "            (process_pdf_file_PyPDF, (input_path, os.path.join(output_path, \"PyPDF.txt\"))),\n",
    "            (process_pdf_file_UnstructuredPDF_hig_res_strategy, (input_path, os.path.join(output_path, \"Unstructured_hi_res.txt\"))),\n",
    "            (process_pdf_file_UnstructuredPDF_default_strategy, (input_path, os.path.join(output_path, \"Unstructured.txt\"))),\n",
    "            (process_pdf_file_UnstructuredPDF_OCR_only_strategy, (input_path, os.path.join(output_path, \"Unstructured_OCR.txt\"))),\n",
    "            (process_pdf_file_PDFMiner, (input_path, os.path.join(output_path, \"PDFMiner.txt\"))),\n",
    "            (process_pdf_file_PDFMiner_as_HTML, (input_path, os.path.join(output_path, \"PDFMiner_HTML.html\"))),\n",
    "            (process_pdf_file_PyMuPDF, (input_path, os.path.join(output_path, \"PyMuPDF.txt\"))),\n",
    "            (process_pdf_file_pdfminerSix, (input_path, os.path.join(output_path, \"pdfminerSix.txt\"))),\n",
    "            (process_pdf_file_textract_with_correction, (input_path, os.path.join(output_path, \"textract\"))),       \n",
    "            (process_pdf_file_llama_index_md, (input_path, os.path.join(output_path, \"llama_index_md.md\"))),\n",
    "            (process_pdf_file_llama_index_txt, (input_path, os.path.join(output_path, \"llama_index:txt.txt\")))\n",
    "        ]\n",
    "\n",
    "        with tqdm(total=len(tasks)) as pbar:\n",
    "            for task in tasks:\n",
    "                func, args = task\n",
    "                pbar.set_description(f\"Processing {args[1]}\")\n",
    "                try:\n",
    "                    func(*args)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nAn error occurred while processing {args[1]}: {str(e)}\")\n",
    "                    print(\"Press any key to continue...\")\n",
    "                    input()\n",
    "                pbar.update()\n",
    "\n",
    "\n",
    "run_all(\"input_data/pdf\", \"output_data\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
